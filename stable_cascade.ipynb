{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nd1QOOSw1OjD"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkshing/notebooks/blob/main/stable_cascade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "# Stable Cascade Demo\n",
        "This notebook is the demo for Stable Cascade aka WÃ¼rstchen v3, from Stability AI on Colab free plan.\n",
        "\n",
        "\n",
        "This was made by [mkshing](https://twitter.com/mk1stats).\n",
        "\n",
        "Visit the following links for the details of the model.\n",
        "- Blog: https://stability.ai/news/introducing-stable-cascade\n",
        "- Paper: https://openreview.net/forum?id=gU58d5QeGv\n",
        "- Code: https://github.com/Stability-AI/StableCascade\n",
        "- HF: https://huggingface.co/stabilityai/stable-cascade\n",
        "- License: [STABILITY AI NON-COMMERCIAL RESEARCH COMMUNITY LICENSE AGREEMEN](https://github.com/Stability-AI/StableCascade/blob/master/LICENSE)\n",
        "\n",
        "\n",
        "*Please remeber that this model was released under non-commercial license.*\n",
        "\n",
        "\n",
        "## Updates\n",
        "### 2024.2.14\n",
        "* Model release. Congratulations on the release to the team! (https://twitter.com/dome_271/status/1757427041563967512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UUjFzR6N1Nx6"
      },
      "outputs": [],
      "source": [
        "#@title Setup\n",
        "!nvidia-smi\n",
        "!pip install git+https://github.com/kashif/diffusers.git@wuerstchen-v3\n",
        "!pip install -U accelerate torch torchvision\n",
        "!pip install gradio==4.17.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rZDSlX_62MYR"
      },
      "outputs": [],
      "source": [
        "#@title Load models\n",
        "import torch\n",
        "from diffusers import StableCascadeDecoderPipeline, StableCascadePriorPipeline\n",
        "device = torch.device(\"cuda\")\n",
        "prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", torch_dtype=torch.bfloat16)\n",
        "decoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", torch_dtype=torch.float16)\n",
        "# prior.enable_model_cpu_offload()\n",
        "prior.to(device)\n",
        "decoder.enable_model_cpu_offload()\n",
        "# torch compile\n",
        "# prior.prior = torch.compile(prior.prior, mode=\"reduce-overhead\", fullgraph=True)\n",
        "# decoder.decoder = torch.compile(decoder.decoder, mode=\"max-autotune\", fullgraph=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GNBbe-r53ina"
      },
      "outputs": [],
      "source": [
        "#@title Run!\n",
        "# original code: https://huggingface.co/spaces/multimodalart/stable-cascade\n",
        "import random\n",
        "import gc\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "MAX_SEED = np.iinfo(np.int32).max\n",
        "MAX_IMAGE_SIZE = 1536\n",
        "\n",
        "\n",
        "def randomize_seed_fn(seed: int, randomize_seed: bool) -> int:\n",
        "    if randomize_seed:\n",
        "        seed = random.randint(0, MAX_SEED)\n",
        "    return seed\n",
        "\n",
        "def generate_prior(prompt, negative_prompt, generator, width, height, num_inference_steps, guidance_scale, num_images_per_prompt):\n",
        "  # prior.to(device=device)\n",
        "  prior_output = prior(\n",
        "      prompt=prompt,\n",
        "      height=height,\n",
        "      width=width,\n",
        "      negative_prompt=negative_prompt,\n",
        "      guidance_scale=guidance_scale,\n",
        "      num_images_per_prompt=num_images_per_prompt,\n",
        "      num_inference_steps=num_inference_steps\n",
        "  )\n",
        "  # prior.to(device=\"cpu\")\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return prior_output.image_embeddings\n",
        "\n",
        "\n",
        "def generate_decoder(prior_embeds, prompt, negative_prompt, generator, num_inference_steps, guidance_scale):\n",
        "\n",
        "  # decoder.to(device=device)\n",
        "  decoder_output = decoder(\n",
        "      image_embeddings=prior_embeds.to(device=device, dtype=decoder.dtype),\n",
        "      prompt=prompt,\n",
        "      negative_prompt=negative_prompt,\n",
        "      guidance_scale=guidance_scale,\n",
        "      output_type=\"pil\",\n",
        "      num_inference_steps=num_inference_steps,\n",
        "      generator=generator\n",
        "  ).images\n",
        "  # decoder.to(device=\"cpu\")\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "  return decoder_output\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(\n",
        "    prompt: str,\n",
        "    negative_prompt: str = \"\",\n",
        "    seed: int = 0,\n",
        "    randomize_seed: bool = True,\n",
        "    width: int = 1024,\n",
        "    height: int = 1024,\n",
        "    prior_num_inference_steps: int = 30,\n",
        "    prior_guidance_scale: float = 4.0,\n",
        "    decoder_num_inference_steps: int = 12,\n",
        "    decoder_guidance_scale: float = 0.0,\n",
        "    num_images_per_prompt: int = 2,\n",
        "):\n",
        "    \"\"\"Generate images using Stable Cascade.\"\"\"\n",
        "    seed = randomize_seed_fn(seed, randomize_seed)\n",
        "    print(\"seed:\", seed)\n",
        "    generator = torch.Generator(device=device).manual_seed(seed)\n",
        "    prior_embeds = generate_prior(\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        generator=generator,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        num_inference_steps=prior_num_inference_steps,\n",
        "        guidance_scale=prior_guidance_scale,\n",
        "        num_images_per_prompt=num_images_per_prompt,\n",
        "\n",
        "    )\n",
        "\n",
        "    decoder_output = generate_decoder(\n",
        "        prior_embeds=prior_embeds,\n",
        "        prompt=prompt,\n",
        "        negative_prompt=negative_prompt,\n",
        "        generator=generator,\n",
        "        num_inference_steps=decoder_num_inference_steps,\n",
        "        guidance_scale=decoder_guidance_scale,\n",
        "    )\n",
        "\n",
        "    return decoder_output\n",
        "\n",
        "\n",
        "examples = [\n",
        "    \"An astronaut riding a green horse\",\n",
        "    \"A mecha robot in a favela by Tarsila do Amaral\",\n",
        "    \"The sprirt of a Tamagotchi wandering in the city of Los Angeles\",\n",
        "    \"A delicious feijoada ramen dish\"\n",
        "]\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  with gr.Column():\n",
        "\n",
        "    prompt = gr.Text(\n",
        "        label=\"Prompt\",\n",
        "        show_label=False,\n",
        "        placeholder=\"Enter your prompt\",\n",
        "    )\n",
        "    run_button = gr.Button(\"Run\")\n",
        "    with gr.Accordion(\"Advanced options\", open=False):\n",
        "        negative_prompt = gr.Text(\n",
        "            label=\"Negative prompt\",\n",
        "            max_lines=1,\n",
        "            placeholder=\"Enter a Negative Prompt\",\n",
        "        )\n",
        "\n",
        "        seed = gr.Slider(\n",
        "            label=\"Seed\",\n",
        "            minimum=0,\n",
        "            maximum=MAX_SEED,\n",
        "            step=1,\n",
        "            value=0,\n",
        "        )\n",
        "        randomize_seed = gr.Checkbox(label=\"Randomize seed\", value=True)\n",
        "        width = gr.Slider(\n",
        "            label=\"Width\",\n",
        "            minimum=1024,\n",
        "            maximum=MAX_IMAGE_SIZE,\n",
        "            step=512,\n",
        "            value=1024,\n",
        "        )\n",
        "        height = gr.Slider(\n",
        "            label=\"Height\",\n",
        "            minimum=1024,\n",
        "            maximum=MAX_IMAGE_SIZE,\n",
        "            step=512,\n",
        "            value=1024,\n",
        "        )\n",
        "        num_images_per_prompt = gr.Slider(\n",
        "            label=\"Number of Images\",\n",
        "            minimum=1,\n",
        "            maximum=2,\n",
        "            step=1,\n",
        "            value=1,\n",
        "        )\n",
        "        prior_guidance_scale = gr.Slider(\n",
        "            label=\"Prior Guidance Scale\",\n",
        "            minimum=0,\n",
        "            maximum=20,\n",
        "            step=0.1,\n",
        "            value=4.0,\n",
        "        )\n",
        "        prior_num_inference_steps = gr.Slider(\n",
        "            label=\"Prior Inference Steps\",\n",
        "            minimum=10,\n",
        "            maximum=30,\n",
        "            step=1,\n",
        "            value=20,\n",
        "        )\n",
        "\n",
        "        decoder_guidance_scale = gr.Slider(\n",
        "            label=\"Decoder Guidance Scale\",\n",
        "            minimum=0,\n",
        "            maximum=0,\n",
        "            step=0.1,\n",
        "            value=0.0,\n",
        "        )\n",
        "        decoder_num_inference_steps = gr.Slider(\n",
        "            label=\"Decoder Inference Steps\",\n",
        "            minimum=4,\n",
        "            maximum=12,\n",
        "            step=1,\n",
        "            value=10,\n",
        "        )\n",
        "  with gr.Column():\n",
        "    result = gr.Gallery(label=\"Result\", show_label=False)\n",
        "\n",
        "  gr.Examples(\n",
        "      examples=examples,\n",
        "      inputs=prompt,\n",
        "      outputs=result,\n",
        "      fn=generate,\n",
        "  )\n",
        "\n",
        "  inputs = [\n",
        "          prompt,\n",
        "          negative_prompt,\n",
        "          seed,\n",
        "          randomize_seed,\n",
        "          width,\n",
        "          height,\n",
        "          prior_num_inference_steps,\n",
        "          prior_guidance_scale,\n",
        "          decoder_num_inference_steps,\n",
        "          decoder_guidance_scale,\n",
        "          num_images_per_prompt,\n",
        "  ]\n",
        "  prompt.submit(\n",
        "      fn=generate,\n",
        "      inputs=inputs,\n",
        "      outputs=result,\n",
        "  )\n",
        "  negative_prompt.submit(\n",
        "      fn=generate,\n",
        "      inputs=inputs,\n",
        "      outputs=result,\n",
        "  )\n",
        "  run_button.click(\n",
        "      fn=generate,\n",
        "      inputs=inputs,\n",
        "      outputs=result,\n",
        "  )\n",
        "\n",
        "  demo.launch(share=True, debug=True, show_error=True)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
